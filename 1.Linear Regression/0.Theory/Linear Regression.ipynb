{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics covered in this notebook:\n",
    "1. What is Linear Regression.\n",
    "2. Simple 1D Linear Regression.\n",
    "3. Quality of Fit - R-Squared.\n",
    "4. Multiple Linear Regression.\n",
    "5. Probabilistic interpretation of the squared error.\n",
    "6. L2 Regularization or Ridge Regression.\n",
    "7. Gradient Descent.\n",
    "8. L1 Regularization or Lasso.\n",
    "9. L1 vs. L2 Regularization.\n",
    "10. References."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a set of questions:\n",
    "1. How to predict future stock price?\n",
    "2. How to predict a rating of a movie?\n",
    "3. How many followers will I get on twitter?\n",
    "4. How can you predict a price of a house?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does all these questions have in common?\n",
    "1. **Outputs**. More specifically continous outputs (price of a house, # of followers etc.). Let's call this **'Y'** or **'y'**\n",
    "2. Predicting continuous outputs is called **regression**\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do I need to predict outputs?\n",
    "1. **Features** or Inputs, Let's call this **'X'**.\n",
    "2. **Training examples**, many X's for which Y's are known.\n",
    "3. A **model**, a function that represents the relationship between X & Y.\n",
    "4. A **loss** or **cost** or **objective** function that tells us how our model represents the training examples.\n",
    "5. **Optimization**, a way we can find the model parameters that minimizes the cost function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple 1D Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's re-state our problem in more general terms:\n",
    "1. We are given a set of points: {(x1,y1),(x2,y2),....(xn,yn)}\n",
    "2. We plot them in a 2D chart.\n",
    "3. We find the line of best fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({\n",
       "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "});"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0,10,50)\n",
    "Y = 2*X + np.random.normal(0,1,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x171cd860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X,Y,c = 'black',label = 'Data')\n",
    "plt.plot(X,2*X,c = 'red',label = 'line of best fit')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our line of best fit is defined as:\n",
    "\\begin{equation}\n",
    " \\large \\hat y_i = ax_i+ b \n",
    "\\end{equation}\n",
    "\n",
    "This is our **model**. <br>\n",
    "\n",
    "What can we do to make sure $\\large y_i$ is closer to $\\large \\hat y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we do this? <br>\n",
    "\n",
    "\\begin{equation}\n",
    "\\large Error = \\Sigma (\\large y_i - \\large \\hat y_i)\n",
    "\\end{equation}\n",
    "\n",
    "No. If the error for a point is -5 and +5 for the other the overall error is zero. However clearly that is not the case.<br>\n",
    "What we want is this:\n",
    "1. For any target != predictions, a +ve contribution to the error.\n",
    "2. Standard way is to square of difference.\n",
    "3. Called the \"sum of the squared errors\".\n",
    "\n",
    "\\begin{equation}\n",
    "\\large Error (E) = \\Sigma (\\large y_i - \\large \\hat y_i)^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have error we have to minimize it. How do we do that?<br> Take derivative & set it to zero! <br>\n",
    "\n",
    "Substitute equation (1) in (3):\n",
    "\n",
    "\\begin{equation}\n",
    "\\large E = \\Sigma (\\large y_i - \\large ax_i- b)^2\n",
    "\\end{equation}\n",
    "<br>\n",
    "\n",
    "We need to minimize E w.r.t a & b. Hence we need to take partial derivative.<br>\n",
    "\n",
    "Derivative of above equation w.r.t a & b:\n",
    "\n",
    "\\begin{align}\n",
    "\\large \\frac{\\partial E}{\\partial a} &= \\Sigma \\,2\\, (\\large y_i - \\large ax_i- b)(-x_i) \\\\\n",
    "\\large \\frac{\\partial E}{\\partial b} &= \\Sigma \\,2\\, (\\large y_i - \\large ax_i- b)\n",
    "\\end{align}\n",
    "<br>\n",
    "\n",
    "Set that to zero: <br>\n",
    "\n",
    "\\begin{align}\n",
    "\\large a\\,\\Sigma \\large x_i^2 + \\large b\\,\\Sigma \\large x_i &= \\Sigma \\,\\large x_i\\,\\large y_i \\\\\n",
    "\\large a\\,\\Sigma \\large x_i + \\large b\\,\\large N &= \\Sigma \\,\\large y_i \\\\\n",
    "\\end{align}\n",
    "<br>\n",
    "\n",
    "Solve those 2 equations simultaneously:\n",
    "\n",
    "\\begin{align}\n",
    "\\large a\\, &= \\frac{N\\,\\large\\Sigma \\,\\large x_i\\,\\large y_i - \\Sigma \\,\\large x_i\\,\\Sigma \\,\\large y_i}{N\\, \\large\\Sigma \\large x_i^2 - (\\Sigma \\large x_i)^2} \\\\\n",
    "\\large b\\, &= \\frac{N\\,\\large\\Sigma \\,\\large x_i\\,\\large y_i^2 - \\Sigma \\,\\large x_i\\,\\Sigma \\,\\large x_i \\large y_i}{N\\, \\large\\Sigma \\large x_i^2 - (\\Sigma \\large x_i)^2} \\\\\n",
    "\\end{align}\n",
    "\n",
    "<br>\n",
    "\n",
    "Simplify it further is diving the above equations by $N^2$ and utilize the defitions of mean:\n",
    "\\begin{align}\n",
    "\\large a\\, &= \\frac{\\overline{\\large xy} - \\overline{\\large x}\\,\\overline{\\large y}}{\\overline{\\large x^2} - \\overline{\\large x}^2}\\\\\n",
    "\\\\\n",
    "\\large b\\, &= \\frac{\\overline{\\large y}\\, \\overline{\\large x^2}  - \\overline{\\large x}\\,\\overline{\\large xy}}{\\overline{\\large x^2} - \\overline{\\large x}^2}\\\\\n",
    "\\end{align}\n",
    "\n",
    "where,\n",
    "\n",
    "\\begin{align}\n",
    "\\large \\overline{x}\\, &= \\frac{1}{N} \\,\\large \\Sigma \\,\\large x_i \\\\\n",
    "\\large \\overline{xy}\\, &= \\frac{1}{N} \\,\\large \\Sigma \\,\\large x_i\\,y_i \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quality of Fit: R-Sq\n",
    "\n",
    "\\begin{equation}\n",
    "\\large R^2 \\, = 1 \\, - \\frac{SS_{\\text{res}}}{SS_{\\text{tot}}} \\\\\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\\begin{align}\n",
    "\\large SS_{\\text{res}}\\, &=  \\large \\Sigma (y\\, - \\hat y_i)^2 \\\\\n",
    "\\large SS_{\\text{tot}}\\, &=  \\large \\Sigma (y\\, - \\overline y_i)^2 \\\\\n",
    "\\end{align}\n",
    "<br>\n",
    "1. Suppose the residual (SS_res) is close to zero. Then the R-Sq will be ~1 -> Perfect correlation. \n",
    "2. Suppose the R-Sq is zero which means SS_res / SS_tot = 1 -> This means we predicted just the everage of y. Model not good.\n",
    "3. When is R-Sq < 0? This means SS_res / SS_tot > 1. Model prediction is worse than mean. Again model not good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error function still doesn't change from 1D regression, only the expression for the prediction does,\n",
    "\n",
    "\\begin{equation}\n",
    "\\large Error (E) = \\Sigma (\\large y_i - \\large \\hat y_i)^2 = \\Sigma (\\large y_i - \\large w^Tx_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "We can still take derivative w.r.t to any component of w: j = 1,....D(No. of inputs or feature dimension) <br>\n",
    "\n",
    "\\begin{align}\n",
    "\\large \\frac{\\partial E}{\\partial w_j} &= \\Sigma 2(\\large y_i - \\large w^Tx_i)(- \\frac{\\partial (w^Tx_i)}{\\partial w_j})\\\\\n",
    "&= \\large \\Sigma \\,2(\\large y_i - \\large w^Tx_i)(- x_{ij})\n",
    "\\end{align}\n",
    "<br>\n",
    "Set this equation to zero. D equations & D unknowns:\n",
    "\n",
    "\\begin{align}\n",
    "\\large \\Sigma \\,2(\\large y_i - \\large w^Tx_i)(- x_{ij}) &= 0\\\\\n",
    "\\large \\Sigma \\,w^Tx_ix_{ij} - \\large \\Sigma y_ix_{ij} &= 0\n",
    "\\end{align}\n",
    "<br>\n",
    "Isolate w & represent everything in matrix form using dot product:\n",
    "<br>\n",
    "\n",
    "\\begin{align}\n",
    "\\large w^T(X^TX)\\, = \\, y^TX\\\\\n",
    "\\end{align}\n",
    "\n",
    "<br>\n",
    "Take transpose on both sides:\n",
    "\n",
    "\\begin{align}\n",
    "\\large [w^T(X^TX)]^T\\, &= \\, [y^TX]^T\\\\\n",
    "\\large (X^TX)w\\, &= \\, X^T\\large y\\\\\n",
    "\\large w \\, &=\\, (X^TX)^{-1}\\, X^T\\large y \n",
    "\\end{align}\n",
    "\n",
    "<br>\n",
    "Use numpy linalg.solve to solve the above equation. Elimiates the need to manually taking inverse:<br>\n",
    "\n",
    "\\begin{align}\n",
    "\\large w \\, &=\\, np.lingalg.solve(X^TX\\,, X^T\\large y)\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Probabilistic Interpretation of Squared Error:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression is the maximum likelihood solution to the line of best fit.\n",
    "\n",
    "What is Maximum Likelihood?\n",
    "1. Assuming Gaussian distribution - If we plot histogram for grades of students we will get a bell curve:<br>\n",
    "<img src=\"Images/bell.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "2. We would do an experiment to collect everyone's grades: {x1, x2,....xn}<br>\n",
    "3. Intuitively, we know the average grade of the students is:\n",
    "\\begin{align}\n",
    "\\large \\mu \\, &= \\frac{1}{N} \\,\\large \\Sigma \\,\\large x_i \\\\\n",
    "\\end{align}\n",
    "4. Is there a systematic way of getting this answer? Suppose we want to find thr true mean of the Gaussian form which the data arises:<br>\n",
    "<br>\n",
    "\\begin{align}\n",
    " find\\, \\large \\mu \\,,\\, where\\,X \\, \\tilde \\, N(\\mu , \\sigma^2) \\\\\n",
    "\\end{align}\n",
    "5. We can write the probability of any single point xi:\n",
    "<br>\n",
    "\\begin{align}\n",
    "\\large p(x_i) \\, &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\,\\large exp^{\\frac {1}{2} \\, \\large \\frac {(\\large x_i-\\ \\large mu)^ 2}{\\large \\sigma^2}}\\\\\n",
    "\\end{align}\n",
    "6. Since we know the grades are iid, we can muliply individual probabiities:\n",
    "\\begin{align}\n",
    "\\large p(x_1,x_2,...x_N) \\, &= \\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\,\\large exp^{\\frac {1}{2} \\, \\large \\frac {(\\large x_i-\\ \\large mu)^ 2}{\\large \\sigma^2}}\\\\\n",
    "\\end{align}\n",
    "7. Another way to write this is as 'likelihood' form. Probability of X given the parameter of interest:\n",
    "   1. We want to find mean so that likelihood is maximized. This is called 'maximum likelihood'.\n",
    "   2. We want to finfd the best setting of mean(mu) so that the date we measured is likely to have come from this distribution.\n",
    "\\begin{align}\n",
    "\\large p(X\\,|\\,\\mu) = \\large p(x_1,x_2,...x_N) \\, &= \\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\,\\large exp^{\\frac {1}{2} \\, \\large \\frac {(\\large x_i-\\ \\large mu)^ 2}{\\large \\sigma^2}}\\\\\n",
    "\\end{align}\n",
    "8. How to find mu? Take log of the likelihood & set it to zero to make it easy to solve. log() in monotomically increasing function so if A > B -> log(A) > log(B)\n",
    "\\begin{align}\n",
    "\\large l \\, &= \\large log \\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\,\\large exp^{\\frac {1}{2} \\, \\large \\frac {(\\large x_i-\\ \\large mu)^ 2}{\\large \\sigma^2}}\\\\\n",
    "\\large l \\, &= \\large [\\, \\Sigma_{i=1}^n-\\frac{1}{2} log(\\sqrt{2\\pi\\sigma^2}) \\,- \\large {\\frac {1}{2} \\, \\large \\frac {(\\large x_i-\\ \\large mu)^ 2}{\\large \\sigma^2}}\\,]\\\\\n",
    "\\end{align}\n",
    "Take derivative and set it to zero:\n",
    "\\begin{align}\n",
    "\\large \\frac{dl}{d\\mu} &= \\large \\Sigma \\, \\frac {(x_i - \\mu)}{\\sigma^2} = 0\\\\\n",
    "\\large \\mu \\, &= \\frac{1}{N} \\,\\large \\Sigma \\,\\large x_i \\\\\n",
    "\\end{align}\n",
    "<br>\n",
    "9. If we look at the equation for l and remove some irrelevant terms that are absorbed by zero, we get:\n",
    "\\begin{align}\n",
    "\\large l \\, &= \\large [\\, \\Sigma_{i=1}^n-\\frac{1}{2} log(\\sqrt{2\\pi\\sigma^2}) \\,- \\large {\\frac {1}{2} \\, \\large \\frac {(\\large x_i-\\ \\large mu)^ 2}{\\large \\sigma^2}}\\,]\\\\\n",
    "\\large equivalent \\,l &= \\large - \\Sigma \\, (x_i - \\mu)^2\\\\\n",
    "\\end{align}\n",
    "\n",
    "### This means maximizing the log-likelihood is equivalent to minimizing negative of squared errors \n",
    "Compare this to our error(E):\n",
    "\\begin{align}\n",
    "\\large \\,l &= \\large - \\Sigma \\, (x_i - \\mu)^2 -> \\,maximize\\\\\n",
    "\\large E & = \\large \\Sigma\\,(\\large y_i - \\large \\hat y_i)^2 -> \\, minimize \\\\\n",
    "\\end{align}\n",
    "So minimizing E is equivalent to maximizing -E. So when we minize the squared error for linear regression, this is equivalent to maximizing the likelihood. <br>\n",
    "Equivalent way of writing this:\n",
    "\\begin{align}\n",
    "\\large y \\,\\tilde \\,\\, N(w^Tx,\\sigma^2)\\\\\n",
    "\\end{align}\n",
    "<br>\n",
    "\\begin{align}\n",
    "\\large y & = \\, \\large w^Tx\\, +\\, \\epsilon,\\, \\epsilon\\, \\tilde\\, N(0,\\sigma^2) \\\\\n",
    "\\end{align}\n",
    "<br>\n",
    "\n",
    "### In other words, linear regression makes the assumption that errors are gaussion and the trend is linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. L2 Regularization or Ridge Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data may have outliers. These outliers are usually measurement errors or data entry errors. In some cases these outliers may actually mean something but in this case lets assume its the former assumption. \n",
    "#### Outliers pull the line of best fit away from the main trend to minimize the squared error. The idea is to ensure the weights are not overly large weights because that might want to fit to outliers.\n",
    "Look at the 2 lines below. Which one is the best fit? Red is the standard model which accounts for the outliers whereas black is the L2 regularization model that ignored the effect of outliers.<br>\n",
    "\n",
    "<img src=\"Images/L2-Reg.png\" alt=\"Drawing\" style=\"length: 1080px;\" style=\"width: 1080px;\"/>\n",
    "\n",
    "### How does L2 Regularization work?\n",
    "Modify the cost function (J) such that large weights are penalized.\n",
    "<br>\n",
    "\n",
    "\\begin{align}\n",
    "\\large J & = \\large \\Sigma\\,(\\large y_i - \\large \\hat y_i)^2 + \\lambda \\lvert \\large w^2\\rvert \\\\\n",
    "\\lvert \\large w^2\\rvert &= \\large w^Tw = \\large w1^2 + w2^2 + ....w_D^2\n",
    "\\end{align}\n",
    "\n",
    "#### Probabilistic Perspective:\n",
    "1. Plain squared error maximizes likelihood because J = negative log likelihood.\n",
    "2. Now we are no longer maximizing this, since there are 2 terms. The 2nd is called prior which has the information about the weights irrespective of the data since it is not dependent on x & y.\n",
    "<img src=\"Images/L2 Prob.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<br>\n",
    "<br>\n",
    "3. Looks like Bayes rule.\n",
    "#### This is called MAP - maximum a posteriori.\n",
    "<img src=\"Images/MAP.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/>\n",
    "<br>\n",
    "<br>\n",
    "4. Solve for w:\n",
    "<img src=\"Images/L2-w.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "<img src=\"Images/L2-der.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "<img src=\"Images/L2-wFinal.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Gradient Descent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Optimization method.\n",
    "2. Used extensively in deep learning.\n",
    "3. Idea: You have a fucntion you want to minimize, J(w) = cost or error. Find optimal inputs to minimize this function.\n",
    "<img src=\"Images/GD.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<br>\n",
    "4. Weight, w = w - learning_rate * dJ/dw.\n",
    "5. Example: J = w^2:\n",
    "   1. dJ/dw = 2w. \n",
    "   2. Set initial  w = 10, learning rate = 0.1.\n",
    "   3. Iteration 1: w = 10 - 0.1 * 20 = 8.\n",
    "   4. Iteration 2: w = 08 - 0.1 * 16 = 6.4.\n",
    "   5. Iteration 3: w = 6.4 - 0.1 * 12.8 = 5.12 and so on..\n",
    "   6. Notice how the weigh 'w' is converging towards the true solution 0. With enough iterations we will coverge.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 5\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "3.2\n",
      "2.56\n",
      "2.048\n",
      "1.6384\n",
      "1.31072\n",
      "1.0485760000000002\n",
      "0.8388608000000002\n",
      "0.6710886400000001\n",
      "0.5368709120000001\n",
      "0.4294967296000001\n",
      "0.3435973836800001\n",
      "0.27487790694400005\n",
      "0.21990232555520003\n",
      "0.17592186044416003\n",
      "0.140737488355328\n",
      "0.11258999068426241\n",
      "0.09007199254740993\n",
      "0.07205759403792794\n",
      "0.057646075230342354\n",
      "0.04611686018427388\n",
      "0.03689348814741911\n",
      "0.029514790517935284\n",
      "0.02361183241434823\n",
      "0.018889465931478583\n",
      "0.015111572745182867\n",
      "0.012089258196146294\n",
      "0.009671406556917036\n",
      "0.007737125245533628\n",
      "0.006189700196426903\n",
      "0.004951760157141522\n",
      "0.003961408125713218\n",
      "0.0031691265005705745\n",
      "0.00253530120045646\n",
      "0.0020282409603651678\n",
      "0.0016225927682921343\n",
      "0.0012980742146337075\n",
      "0.001038459371706966\n",
      "0.0008307674973655728\n",
      "0.0006646139978924582\n",
      "0.0005316911983139665\n",
      "0.00042535295865117324\n",
      "0.0003402823669209386\n",
      "0.00027222589353675085\n",
      "0.0002177807148294007\n",
      "0.00017422457186352054\n",
      "0.00013937965749081642\n",
      "0.00011150372599265314\n",
      "8.920298079412252e-05\n",
      "7.136238463529802e-05\n",
      "Notice how w is converging towards zero!\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    w -= learning_rate * 2* w\n",
    "    print(w)\n",
    "print('Notice how w is converging towards zero!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent for Linear Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function to minimize:\n",
    "\\begin{align}\n",
    "\\large J \\, &= \\large (Y\\, - Xw)^T (Y\\, - Xw) \\\\\n",
    "\\end{align}\n",
    "\n",
    "<br>\n",
    "Gradient:\n",
    "\\begin{align}\n",
    "\\large \\frac {\\partial J}{\\partial w} \\, &= \\large -2X^TY\\, +\\, 2X^TXw \\,=\\, 2X^T(\\hat Y\\, -\\, Y) \\\\\n",
    "\\end{align}\n",
    "<br>\n",
    "#### Instead of setting it to 0 & solving for w, we will just take small steps in this direction.\n",
    "\n",
    "So weight update will be for a certain number of steps: <br>\n",
    "\\begin{align}\n",
    "\\large w \\, &= \\large w - \\eta * X^T(\\hat Y\\, -\\, Y) \\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. L1 Regularization (Lasso):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general we want no. of features(D) << no. of samples(N). Also, there might be a case where some of the features are just constant or quasi-const, uncorrelated to output etc. In all these cases we could use L1 regularization to eliminate those features from the model.\n",
    "\n",
    "1. Select a small number of important features that actually predict the trend.\n",
    "2. Eliminate the noise influence on the output.\n",
    "3. Similar to L2. It has a penalty term using L1 norm.\n",
    "<img src=\"Images/L1-Concept.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/>\n",
    "4. Similar to L2 this also puts a prior on w, so its also a MAP estimation of w. We had a gaussian distribution on the prior for L2. We have Laplace here:\n",
    "\\begin{align}\n",
    "\\large p(w) \\, &= \\large \\frac {\\lambda}{2} exp(- \\lambda \\lvert \\large w\\rvert) \\\\\n",
    "\\end{align}\n",
    "5. Taking derivative of cost function:\n",
    "\\begin{align}\n",
    "\\large \\frac {\\partial J}{\\partial w} \\, &= \\large -2X^TY\\, +\\, 2X^TXw \\,+\\, \\lambda sign(w) \\\\\n",
    "\\end{align}\n",
    "6. Since this has a sign term we can't used closed form solution to update weights and use gradient descent instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. L1 vs L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 reduces the effect of outliers on the model by maintaining the weights to a smaller value. \n",
    "L1 removes unwanted features. Encourages a sparse solution.\n",
    "\n",
    "Both helps prevent overfitting, by not fitting to noise. \n",
    "\n",
    "L2 penalty is quadratic: as w -> 0 derivative ->0  <br>\n",
    "L1 penalty is abs func : as w -> 0 doesn't matter it will fall at a contant rate. When it reaches 0, is stays there forever.\n",
    "\n",
    "#### Combine L1 & L2 = ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.References:\n",
    "1. An Introduction to Statistical Learning Textbook by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.\n",
    "2. University of Michigan EECS 445 - Machine Learning Course (https://github.com/eecs445-f16/umich-eecs445-f16).<br>\n",
    "3. University of Toronto CSC 411 - Intro. to Machine Learning (http://www.cs.toronto.edu/~urtasun/courses/CSC411_Fall16/CSC411_Fall16.html).<br>\n",
    "4. Stanford CS109 - Intro. to proabability for computer scientists (https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/). <br>\n",
    "5. Few online courses on Udemy, Coursera etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
