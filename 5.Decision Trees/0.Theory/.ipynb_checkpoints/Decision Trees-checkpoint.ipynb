{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics covered in this notebook:\n",
    "1. What are Decision Trees?\n",
    "2. What is Information Entropy?\n",
    "3. What is Information Gain?\n",
    "4. How do we choose best split?\n",
    "5. What makes a good tree?\n",
    "6. Decision Trees vs KNN.\n",
    "7. Applications.\n",
    "8. References."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Decision Trees:\n",
    "\n",
    "Simple in concept, complicated to implement.\n",
    "\n",
    "1. Pick an attribute, do a simple test.\n",
    "2. Conditioned on choice, pick another attribute, do another test.\n",
    "3. In the leaves, assign a class with majority vote.\n",
    "4. Do the other branches as well.\n",
    "5. Gives axis aligned decision boundaries.\n",
    "\n",
    "Basically: A bunch of nested if-statements.\n",
    "\n",
    "Example - Spam Classifier:\n",
    "\n",
    "$If (doc\\,contains\\,'money'\\,):$<br>\n",
    "    $\\quad If\\,(doc\\,contains\\,'free'\\,):$<br>\n",
    "        $\\qquad Return\\,\\,True$ <br>\n",
    "    $\\quad else:$<br>\n",
    "        $\\qquad Return\\,\\,False$ <br>\n",
    "$else:$<br>\n",
    "    $\\quad Return\\,\\,False$ <br>\n",
    "    \n",
    "1. One key feature is that we look at one attribure at a time.\n",
    "    1. Each condition checks only one column of X.\n",
    "    2. Attributes = 'Input Features'.\n",
    "    3. Ex:\n",
    "        1. If (height < 5): Go to left node Else: Go to right node.\n",
    "\n",
    "2. What does it tell us about the geometry of the problem?\n",
    "    1. Splits are always orthogonal to the axes.\n",
    "    2. Whereas discriminating line can be at an angle - Ex. Linear Classifier.\n",
    "    3. Can still get a highly non-linear boundary - if we split multiple times, splits at each level.\n",
    "    \n",
    "3. Recursiveness- Because its a Tree!.\n",
    "    1. Each node is a TreeNode object.\n",
    "    2. But its children are also TreeNode objects.\n",
    "    3. Leaf nodes have no children.\n",
    "    4. Leaf nodes are where we make predictions.\n",
    "    5. It then bubbles back up to the root node.\n",
    "    \n",
    "What makes this ML?\n",
    "\n",
    "1. Its how we choose the conditions.\n",
    "    1. Based on Information theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Information Entropy:\n",
    "\n",
    "High level: We want to choose a split that maximizes reduction in uncertainity.<br>\n",
    "\n",
    "Example: Going from 50% certain to 100% certain is better than going from 50% to 75%\n",
    "\n",
    "1. Related to variance.\n",
    "    1. Wide variance: More uncertainity. \n",
    "    2. Slim variance: Less uncertainity. <img src=\"Images/Variance.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "2. Entropy: \n",
    "    1. It is always positive. Since p is between 0-1 and negative log is also positive.\n",
    "    2. Entropy - We always mean log base 2 implicity.\n",
    "\n",
    "$\\qquad H(X) = -\\displaystyle\\sum_{x} p(x)\\log p(x).$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Random variable:\n",
    "Example:<br>\n",
    "P(X = 1) = p<br>\n",
    "P(X = 0) = 1 - p<br>\n",
    "\n",
    "$$Entropy\\,H(p)\\,=\\,-plog(p)\\,-\\,(1-p)log(1-p)$$<br>\n",
    "What value of p maximizes p? Solve dH/dp = 0 for p.<br>\n",
    "Answer: p = 0.5\n",
    "\n",
    "<img src=\"Images/EntropyVsP.png\" alt=\"Drawing\" style=\"width: 250px;\"/>\n",
    "\n",
    "1. If p = 0.5, there is no possible way to make a good prediction, we'll always have a probability of 50% being wrong.\n",
    "2. If p = 0.8, then we should always predict 1 because that gives us the best chance of being correct.\n",
    "3. Entropy is a measure of how much information we get from finding out the value of the random variable.\n",
    "    1. If we flip a coint with p = 0.8 and we get heads(1), we don't gain much information, we were already 80% certain.\n",
    "    2. If we flip a coint with p = 0.5 and we get heads, we gain maximium amount of information we could have.\n",
    "    3. Prior to knowing this, we were maximally clueless about the value we would get.\n",
    "    4. In general, uniform distribution yields maximum entropy."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Information Gain:\n",
    "\n",
    "What is Information Gain?\n",
    "1. Suppose we have labels 0,0,1,1.\n",
    "2. H(Y) = 1.\n",
    "3. Suppose we have an attribute that splits the data perfectly.\n",
    "    1. Ex: if X > 0, Y = 0 ,if X < 0, Y = 1.\n",
    "    2. Then we have the left nodes: Y_left = {0,0}.\n",
    "    3. Right nodes: Y_right = {1,1}.\n",
    "    4. The entropy for each subset of data is 0.\n",
    "    5. Information Gain, IG(Y|Split on X) = H(Y) - 0.5* H(Y_left) - - 0.5* H(Y_right) = 1 - 0 - 0 = 1.\n",
    "        1. 0.5 indicates half the data went to the left and half to the right.\n",
    "        2. Ensures IG>=0.\n",
    "        \n",
    "Another Example:\n",
    "![image.png](attachment:image.png)\n",
    "1. H(Y) = 1.\n",
    "2. Split X1:\n",
    "    1. X1 = 1: H(Y|X1=1) = - (3/4)log2(3/5) - (1/4)log2(1/4) = 0.811.\n",
    "    2. X1 = 2: H(Y|X1=2) = - (2/6)log2(2/6) - (4/6)log2(4/6) = 0.918.\n",
    "    3. IG = 1 - (4/10) * 0.811 - (6/10) * 0.918 = 0.1248.\n",
    "3. Split X2:\n",
    "    1. X2 = 5: H(Y|X2=5) = 1.\n",
    "    2. X2 = 10: H(Y|X2=10) = 1.\n",
    "    3. IG = 1 - (4/10) * 1 - (6/10) * 1 = 0.\n",
    "\n",
    "Since splitting across X1 gives maximum IG, we should split on X1 first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Choosing the best split:\n",
    "1. In the example above X1 & X2 had only 2 values.\n",
    "2. But in datasets like MNIST, the data is continuous. How do we pick the best split?\n",
    "    1. Continuous data -> Infinite places to split.\n",
    "    2. Need some way to find smalles number of splits.\n",
    "    \n",
    "Rules:<br>\n",
    "\n",
    " X : 0 1 2 3<br>\n",
    " Y : 1 0 1 0<br>\n",
    " \n",
    "We only need to consider the midpoint between any two sorted X's.<br>\n",
    "Split between 1,2 -> 1.5 -> Entropy = 1<br>\n",
    "Split between 1,2 -> 1.75 -> Entropy still = 1<br>\n",
    "\n",
    "Only need to consider boundaries between differing labels:<br>\n",
    "\n",
    "Y: 1,1,0,0,0,0,0,0,1,1,1,1,1,1,0,0\n",
    "\n",
    "Split at the middle -> total entropy = 2 * (-(6/8)log2(6/8) - (2/8)log2(2/8)) = 1.62.<br>\n",
    "Move one left       -> total entropy = 1.78.<br>\n",
    "Move one more left  -> total entropy = 1.89.<br>\n",
    "\n",
    "Further from boundary -> higher entropy -> lower information gain.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Split Algorithm:\n",
    "1. Sort X's for current column in order, sort Y in the corresponding way.\n",
    "2. Find all the boundary points where Y changes from one value to another.\n",
    "3. Calculate information gain when splitting at each boundary.\n",
    "4. Keep the split that yields max. information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What makes a good tree?\n",
    "1. Not too small: need to handle important but possible subtle disctinctions in data.\n",
    "2. Not too big:\n",
    "    1. Computational efficiency - avoid redundant attributes.\n",
    "    2. Avoid over-fitting training examples.\n",
    "3. **Occam's Razor**: Find the simples hypothesis (smallest tree) that fits the observations.\n",
    "4. Inductive bias: small trees with informative nodes near the root.\n",
    "5. In practice, one often regilarizes the construction process to try to get small but highly informative trees.\n",
    "\n",
    "### Problems:\n",
    "1. Exponentially less data at lower nodes.\n",
    "2. Too big of a tree can overfit the data.\n",
    "3. Greedy algoritms don't necessarily yield the global optimum.\n",
    "4. Not suited for continous attributes.\n",
    "5. Bad on parity(XOR) & majority fucntions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Decision Trees vs. KNN:\n",
    "1. Decision boundaries are axis-alignes, tree structures whereas in KNN it is piecewise linear.\n",
    "2. Test complexity: Attributes & splits whereas in KNN it is non-parametric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Applications:\n",
    "1. Used in XBOX to classify body parts. Depth image -> body parts -> 3D joint proposals.\n",
    "2. Flight simulators: 20 state variables, 90K examples based on expert pilot's actions; auto-pilot tree.\n",
    "3. Yahoo's ranking challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. References:\n",
    "1. An Introduction to Statistical Learning Textbook by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.\n",
    "2. University of Michigan EECS 445 - Machine Learning Course (https://github.com/eecs445-f16/umich-eecs445-f16).<br>\n",
    "3. University of Toronto CSC 411 - Intro. to Machine Learning (http://www.cs.toronto.edu/~urtasun/courses/CSC411_Fall16/CSC411_Fall16.html).<br>\n",
    "4. Stanford CS109 - Intro. to proabability for computer scientists (https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/). <br>\n",
    "5. Few online courses on Udemy, Coursera etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
