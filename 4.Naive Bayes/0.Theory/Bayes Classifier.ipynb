{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics covered in this notebook:\n",
    "1. Types of Classifiers - Generative vs. Discriminative.\n",
    "2. Bayes Rule.\n",
    "3. Bayes Classifiers.\n",
    "4. Naive Bayes.\n",
    "5. Naive Bayes vs. KNN.\n",
    "6. Revisit Bayes Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Types of Classifiers:\n",
    "Given inputs **x** and classes **y** we can do classification in several ways:\n",
    "1. **Discriminative Classifiers**: These classifiers estimate parameters of decision boundary/class sepratoe from labeled examples.\n",
    "    1. Learn mappings directly from space of inputs X to class labels {0, 1, 2,....., K}. For example:\n",
    "        1. Linear Regression as a classifier.\n",
    "        2. Neural Networks.\n",
    "    2. Learn p(y|x) directly. For example:\n",
    "        1. Logistic Regression.\n",
    "2. **Generative Classifiers**: Model the distribution of inputs characteristic of the class.\n",
    "    1. These classifiers try to model p(x|y).\n",
    "    2. Apply Bayesrile for classification. Thus called **Bayes Classifiers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({\n",
       "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "});"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bayes Rule:\n",
    "Given **x** we want to compute class(C) probabilities using Bayes Rule:\n",
    "<br>\n",
    "<br>\n",
    "\\begin{equation}\n",
    " \\large p(C \\,|\\, x) \\,=\\, \\frac {p(x\\,|\\,C)p(C)}{p(x)}\n",
    "\\end{equation}\n",
    "<br>\n",
    "<br>\n",
    "More formally:\n",
    "<br>\n",
    "<br>\n",
    "\\begin{equation}\n",
    " \\large posterior \\,=\\, \\frac {Class likelihood \\,\\,*\\,\\, prior}{Evidence}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bayes Classifier:\n",
    "\n",
    "1. Determine if an email is spam:\n",
    "    1. Can look at words like **'free','pills','money'**, etc.\n",
    "    2. Find:\n",
    "        1. p(money|spam).\n",
    "        2. p(money|not spam).\n",
    "        \n",
    "How do we find these?\n",
    "\n",
    "Discrete probabilities are just counts. For example,\n",
    "\n",
    "$$ \\large p(money\\,|\\,spam) = \\frac {\\large count\\,\\,(spam\\,\\,messages\\,\\,containing\\,\\,'money\\,')} {\\large\\,count\\,(spam\\,\\,messages)}$$\n",
    "\n",
    "Similarly we can calculate p(money|not spam). This looks a lot like a Bayes rule.\n",
    "\n",
    "### What makes this Bayesian?\n",
    "Let's consider a spam classifier as an example.\n",
    "1. We want p(spam|X):\n",
    "    1. Apply Bayes rule.\n",
    "    2. **p(spam|X) = p(X|spam) * p(spam)/ p(X)** where p(spam) = class prior, p(X|spam) is likelihood & p(spam|X) is posterior.\n",
    "    3. Similarly we calculate p(not spam|X).\n",
    "        1. We classify based on what is bigger.\n",
    "        2. p(spam|X) > p(not spam|X) -> Spam!\n",
    "        3. p(spam|X) < p(not spam|X) -> Not spam!\n",
    "    4. Y = argmax{p(C|X)} = argmax{p(X|C) * p(C)}, where p(X) can be ignored as it is independent of C.\n",
    "        1. For example: 10 spam emails, 20 not spam emails:\n",
    "            1. p(spam) = 1/3.\n",
    "            2. p(not spam) = 2/3.\n",
    "            \n",
    "Before looking more into Bayes classifiers let's look with Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Naive Bayes(NB):\n",
    "\n",
    "Consider **p(cash|spam)**. Is it correlated with **p(money|spam)**? \n",
    "Probably. But if we assume those are independent it is called '**Naive Bayes**'.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling P(X|C)\n",
    "\n",
    "P(X|C) = P(words|C). All words are independent hence we can multiply the probabilities.<br>\n",
    "\n",
    "P(words|C) = P(w|C) * (1 - P(w|c)) where w is each word from the set words.<br>\n",
    "\n",
    "### NB Implementation - Use Gaussian Distribution.\n",
    "\n",
    "1. We won't use full covaraince matrix since all dimensions are independent in NB.<br>\n",
    "\n",
    "2. Cov(i,j) = E[ (x_i - mu_i)(x_j - mu_j) ] ( =0 if x_i is independent of x_j).<br>\n",
    "\n",
    "3. Cov(i,i) = var(x_i) = sigma^2.\n",
    "    1. This is called axis aligned elliptical covariance.\n",
    "    2. Instead of DxD covariance matrix store a D sized vector.\n",
    "    3. Scipy allows us to pass in either.\n",
    "    \n",
    "4. Effectively still doing:\n",
    "    1. p(X|C) = p(x1|C) p(x2|C)......p(xn|C)\n",
    "    2. p(X|C) = N(x1; mu1, var1-sq) N(x2; mu2, var2-sq).....N(xn; mun, varn-sq) ->multivariate gaussian.\n",
    "    \n",
    "5. Exponential slows down. Hence use log probabilities. Scipy has a function to calculate log probabilites too.\n",
    "    1. Prediction = argmax{p(X|C) p(C)} --> argmax{log p(X|C) + log p(C)}.\n",
    "6. Smoothing:\n",
    "    1. Singular covariance problem - matrix equivalent of divison by zero.\n",
    "    2. Add smoothing.\n",
    "        1. MLE = transpose((X - mu)) * (X - mu)/(N-1).\n",
    "        2. Smoothed MLE = transpose((X - mu)) * (X - mu)/(N-1) + a * I, where is a very small number (0.0001). -> Adds numerical\n",
    "           stability.\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Naive Bayes vs. KNN:\n",
    "\n",
    "1. Concept is almost opposite of KNN.\n",
    "    1. KNN: We approximate some function f(words in document) -> spam/not spam.\n",
    "    2. NB : Assume data arises/ produced from the target label.\n",
    "        1. Spam -> Spammy document -> model p(document|spam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Revisit Bayes Classifier:\n",
    "\n",
    "In section 4 where NB implementation is discussed we assumed that the off-diagonal elements in the covariance matrix is zero. If it is non-zero it is referred to as **Bayes Classifier**.\n",
    "\n",
    "### Advantages:\n",
    "1. Grounded in probability, which can be powerful.\n",
    "2. Each variable is modeled explicitly, change model of p(x|C) if the result is poor.\n",
    "    1. You know exactly how each variable affects results.\n",
    "\n",
    "\n",
    "### Disadvantages:\n",
    "1. Historically discrimative models have worked better. Ex. Deep learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
