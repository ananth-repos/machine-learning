{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics covered in this notebook:\n",
    "1. What is Bayes Classifier?\n",
    "2. Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Classifier:\n",
    "\n",
    "1. Grounded in probabilty, which can be powerful.\n",
    "2. Start with 'naive'Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "1. Determine if an email is spam:\n",
    "    1. Can look at words like 'free','pills','money', etc.\n",
    "    2. Find:\n",
    "        1. p(money|spam).\n",
    "        2. p(money|not spam).\n",
    "        \n",
    "How do we find these?\n",
    "\n",
    "Discrete probabilities are just counts. For example,\n",
    "\n",
    "$$ \\large p(money\\,|\\,spam) = \\frac {\\large count\\,(spam\\,messages\\,containing\\,\\,'money\\,')} {\\large\\,count\\,(spam\\,messages)}$$\n",
    "\n",
    "Similarly for p(money|not spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What makes this naive?\n",
    "\n",
    "Consider p(cash|spam). Is it correlated with p(money|spam)? <br>\n",
    "Probably. But we assume indepence. Hence 'naive'.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What makes this Bayesian?\n",
    "1. We want p(spam|X):\n",
    "    1. Apply Bayes rule.\n",
    "    2. p(spam|X) = p(X|spam) * p(spam)/ p(X) where p(spam) = class prior, p(X|spam) is likelihood & p(spam|X) is posterior.\n",
    "    3. Similarly we calculate p(not spam|X).\n",
    "        1. We classify based on what is bigger.\n",
    "        2. p(spam|X) > p(not spam|X) -> Spam!\n",
    "        3. p(spam|X) < p(not spam|X) -> Not spam!\n",
    "    4. Y = argmax{p(C|X)} = argmax{p(X|C) * p(C)}, where p(X) can be ignored as it is independent of C.\n",
    "        1. For example: 10 spam emails, 20 not spam emails:\n",
    "            1. p(spam) = 1/3.\n",
    "            2. p(not spam) = 2/3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling P(X|C)\n",
    "\n",
    "P(X|C) = P(words|C). All words are independent hence we can multiply the probabilities.<br>\n",
    "\n",
    "P(words|C) = P(w|C) * (1 - P(w|c)) where w is each word from the set words.<br>\n",
    "\n",
    "Vocabulary = words in mail U words not in doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes vs. KNN:\n",
    "\n",
    "1. Concept is almost opposite of KNN.\n",
    "    1. KNN: We approximate some function f(words in document) -> spam/not spam.\n",
    "    2. NB : Assume data arises/ produced from the target label.\n",
    "        1. Spam -> Spammy document -> model p(document|spam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB Implementation - Use Gaussian Distribution.\n",
    "\n",
    "1. We won't use full covaraince matrix since all dimensions are independent in NB.<br>\n",
    "\n",
    "2. Cov(i,j) = E[ (x_i - mu_i)(x_j - mu_j) ] ( =0 if x_i is independent of x_j).<br>\n",
    "\n",
    "3. Cov(i,i) = var(x_i) = sigma^2.\n",
    "    1. This is called axis aligned elliptical covariance.\n",
    "    2. Instead of DxD covariance matrix store a D sized vector.\n",
    "    3. Scipy allows us to pass in either.\n",
    "    \n",
    "4. Effectively still doing:\n",
    "    1. p(X|C) = p(x1|C) p(x2|C)......p(xn|C)\n",
    "    2. p(X|C) = N(x1; mu1, var1-sq) N(x2; mu2, var2-sq).....N(xn; mun, varn-sq) ->multivariate gaussian.\n",
    "    \n",
    "5. Exponential slows down. Hence use log probabilities. Scipy has a function to calculate log probabilites too.\n",
    "    1. Prediction = argmax{p(X|C) p(C)} --> argmax{log p(X|C) + log p(C)}.\n",
    "6. Smoothing:\n",
    "    1. Singular covariance problem - matrix equivalent of divison by zero.\n",
    "    2. Add smoothing.\n",
    "        1. MLE = transpose((X - mu)) * (X - mu)/(N-1).\n",
    "        2. Smoothed MLE = transpose((X - mu)) * (X - mu)/(N-1) + a * I, where is a very small number (0.0001). -> Adds numerical\n",
    "           stability.\n",
    "                        \n",
    "                           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
