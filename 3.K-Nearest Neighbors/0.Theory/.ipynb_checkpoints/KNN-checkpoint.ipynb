{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics covered in this notebook:\n",
    "1. What is K-Nearest Neighbors(kNN) mean?\n",
    "2. Implementation.\n",
    "3. How to choose K?\n",
    "4. Common Issues & Fix.\n",
    "5. Where kNN can fail?\n",
    "6. References."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. K - Nearest Neighbors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The idea is to make prediction using the closest know data points.\n",
    "2. Look at the image below:\n",
    "    1. There are 2 categories: Star & Triangle.\n",
    "    2. Consider a new test data point -> Green square. What is this point classified as?\n",
    "        1. K = 3 -> 3-nearest neighbor -> Pick star.\n",
    "        2. K = 5 -> 5-nearest neighbor -> Pick traingle.\n",
    "        3. We classify by calcualting eulidean distance between nearby K points & their corresponding classes.\n",
    "3. Forms complex decision boundaries; adapts to data density.\n",
    "4. These type of models are called non-parametric models.\n",
    "5. These type of classifiers are also called as lazy classifiers.\n",
    "    1. train(X,Y) doesn't do anything. Just stores X & Y.\n",
    "    2. predict(X') does all the work by looking through stored X & Y.\n",
    "<img src=\"Images/knn.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "<br>\n",
    "6. Few assumptions:\n",
    "    1. Output varies smoothly with input.\n",
    "    2. Data occupies sub-space of high-dimensional input space.\n",
    "    \n",
    "\n",
    "## 2. Implementation:\n",
    "3. Idea is simple but implementation can be tricky.\n",
    "4. Keeping track of arbitrary number of distances not so easy.\n",
    "5. First -> need to look through all of the training data -> O(N).\n",
    "6. Then need to look through the closest distances you have stored so far -> O(K).\n",
    "\\begin{align}\n",
    "\\large \\lvert\\lvert \\large x^{a} \\,-\\, x^{b}\\rvert\\rvert_2 \\, &= \\large \\sqrt {\\Sigma_{j=1}^d (x_j^{a} \\,-\\, x_j^{b})^2} \\\\\n",
    "\\end{align}\n",
    "7. Total O(NK).\n",
    "    8. Searching through a sorted list would be O(log K), a little better.\n",
    "    9. Even better: Ball Tree, K-D Tre etc.\n",
    "8. Once we have the k-nearest neighbors we need to turn them into votes which means we need to store the class as well.\n",
    "    9. {dist1: class1, dist2:class2,...}\n",
    "9. Count up the values:\n",
    "    1. {class1: num_class1,class2:num_class2...}\n",
    "10. Pick the class that has the highest votes.\n",
    "    1. What if there is tie?\n",
    "        1. Use whatever argmax(votes) outputs.\n",
    "        2. Pick one at random.\n",
    "        3. Weight by distance to neighbors.\n",
    "        \n",
    "## 3. How to choose K?:\n",
    "1. No easy answer.\n",
    "2. K is hyperparameter.\n",
    "3. Use cross-validation.\n",
    "4. Larger k may lead to better performance.\n",
    "5. But if we set k too large we may look at samples that are not neighbors.\n",
    "6. Rule of thumb: k < sqrt(n), where n is number of training examples.\n",
    "\n",
    "## 4. Common Issues & Fix:\n",
    "1. If some attribute have larger ranges, they are treated as more important:\n",
    "    1. Normalize scale:\n",
    "        1. Linear transformatio to be between [0,1].\n",
    "        2. Scale to have 0 mean and 1 variance.\n",
    "        3. Caution: Sometimes scale matters.\n",
    "2. Irrelevant attributes can add noise to distance measure. \n",
    "    1. Remove attributes.\n",
    "    2. Adapt weights using regularization techniques.\n",
    "3. Computation:O(NK)\n",
    "    1. Use subset of dimensions.\n",
    "    2. Pre-sort training examples into fast data structures(e.g. kd-trees) - Need to read about it.\n",
    "    3. Compute only approximate distance(e.g. LSH).\n",
    "    4. Remove redundant data(e.g., condensing).\n",
    "4. High Dimensional Data: 'Curse of dimensionality'\n",
    "    1. Required amount of data increases exponentially with dimension.\n",
    "    2. Computation cost also increases.\n",
    "\n",
    "## 5. Where kNN can fail?\n",
    "1. Grid of alteranting dots.\n",
    "    1. If you choose K=3, there will always 2/3 vote from wrong class.\n",
    "        1. Can fix by choosing K = 1.\n",
    "        2. Weighing each points by distance.\n",
    "<img src=\"Images/knn_fail.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "## 6. References:\n",
    "\n",
    "1. An Introduction to Statistical Learning Textbook by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.\n",
    "2. University of Michigan EECS 445 - Machine Learning Course (https://github.com/eecs445-f16/umich-eecs445-f16).<br>\n",
    "3. University of Toronto CSC 411 - Intro. to Machine Learning (http://www.cs.toronto.edu/~urtasun/courses/CSC411_Fall16/CSC411_Fall16.html).<br>\n",
    "4. Stanford CS109 - Intro. to proabability for computer scientists (https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/). <br>\n",
    "5. Few online courses on Udemy, Coursera etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
